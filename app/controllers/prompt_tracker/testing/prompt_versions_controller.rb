# frozen_string_literal: true

module PromptTracker
  module Testing
    # Controller for viewing prompt versions in the Testing section
    class PromptVersionsController < ApplicationController
    before_action :set_prompt, except: [ :generate_tests ]
    before_action :set_version, only: [ :show, :compare, :activate ]
    before_action :set_version_standalone, only: [ :generate_tests ]

    # Make path helpers available to views
    helper_method :load_more_runs_path, :run_test_path, :datasets_path

    # GET /prompts/:prompt_id/versions/:id
    # Show version details with tests
    def show
      @tests = @version.tests.includes(:test_runs).order(created_at: :desc)

      # Calculate metrics from test runs (not from llm_responses which are production tracked calls)
      test_runs = TestRun.joins(:test).where(prompt_tracker_tests: { testable: @version })
      @total_calls = test_runs.count
      @avg_response_time = test_runs.average(:execution_time_ms)
      @total_cost = test_runs.sum(:cost_usd)

      # Calculate average score from evaluations on test runs
      test_evaluations = Evaluation.where(test_run: test_runs)
      @avg_score = test_evaluations.any? ? test_evaluations.average(:score) : nil

      # Calculate test pass/fail counts
      @tests_passing = @tests.select(&:passing?).count
      @tests_failing = @tests.reject(&:passing?).count
      @total_tests = @tests.count
    end

    # GET /prompts/:prompt_id/versions/:id/compare
    # Compare this version with another
    def compare
      # Get comparison version (from params or default to previous version)
      if params[:compare_with].present?
        @compare_version = @prompt.prompt_versions.find(params[:compare_with])
      else
        # Default to previous version
        @compare_version = @prompt.prompt_versions
                                  .where("version_number < ?", @version.version_number)
                                  .order(version_number: :desc)
                                  .first
      end

      if @compare_version
        # Calculate metrics for both versions
        @version_metrics = calculate_version_metrics(@version)
        @compare_metrics = calculate_version_metrics(@compare_version)

        # Calculate differences
        @metrics_diff = {
          calls: @version_metrics[:calls] - @compare_metrics[:calls],
          avg_response_time: @version_metrics[:avg_response_time].to_f - @compare_metrics[:avg_response_time].to_f,
          total_cost: @version_metrics[:total_cost].to_f - @compare_metrics[:total_cost].to_f,
          avg_score: (@version_metrics[:avg_score].to_f - @compare_metrics[:avg_score].to_f).round(2)
        }
      end

      # Get all versions for comparison dropdown
      @all_versions = @prompt.prompt_versions.order(version_number: :desc)
    end

    # POST /prompts/:prompt_id/versions/:id/activate
    # Activate this version and deprecate all others
    def activate
      unless @version.draft? || @version.deprecated?
        redirect_to testing_prompt_prompt_version_path(@prompt, @version), alert: "Version is already active."
        return
      end

      @version.activate!
      redirect_to testing_prompt_prompt_version_path(@prompt, @version), notice: "Version activated successfully."
    end

    # POST /testing/versions/:id/generate_tests
    # Generate tests with AI for this prompt version
    def generate_tests
      result = TestGeneratorService.generate(
        prompt_version: @version,
        instructions: params[:instructions].presence
      )

      redirect_to testing_prompt_prompt_version_path(@version.prompt, @version),
                  notice: "Generated #{result[:count]} test(s) successfully."
    rescue TestGeneratorService::MalformedResponseError => e
      Rails.logger.error "[TestGeneratorService] Malformed response: #{e.message}"
      redirect_to testing_prompt_prompt_version_path(@version.prompt, @version),
                  alert: "AI test generation failed: #{e.message}"
    end

    private

    def set_prompt
      @prompt = Prompt.find(params[:prompt_id])
    end

    def set_version
      @version = @prompt.prompt_versions.includes(:llm_responses).find(params[:id])
    end

    # Set version without requiring prompt_id (for standalone routes)
    def set_version_standalone
      @version = PromptVersion.find(params[:id])
    end

    def calculate_version_metrics(version)
      # Get metrics from test runs (not from llm_responses which are production tracked calls)
      test_runs = TestRun.joins(:test).where(prompt_tracker_tests: { testable: version })
      test_evaluations = Evaluation.where(test_run: test_runs)

      {
        calls: test_runs.count,
        avg_response_time: test_runs.average(:execution_time_ms) || 0,
        total_cost: test_runs.sum(:cost_usd) || 0,
        avg_score: test_evaluations.any? ? test_evaluations.average(:score) : 0
      }
    end

    # Helper method for generating load_more_runs path
    def load_more_runs_path(test, offset:, limit:)
      load_more_runs_testing_prompt_version_test_path(@version, test, offset: offset, limit: limit)
    end

    # Helper method for generating run_test path
    def run_test_path(test)
      run_testing_prompt_version_test_path(@version, test)
    end

    # Helper method for generating datasets path
    def datasets_path
      testing_prompt_prompt_version_datasets_path(@prompt, @version)
    end
    end
  end
end
