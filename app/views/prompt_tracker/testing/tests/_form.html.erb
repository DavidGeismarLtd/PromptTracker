<% test ||= @test %>
<% in_modal ||= false %>

<%= form_with(model: test,
              url: form_url,
              local: !in_modal,
              data: in_modal ? { turbo_stream: true } : {}) do |f| %>
  <% if test.errors.any? %>
    <div class="alert alert-danger">
      <h5><%= pluralize(test.errors.count, "error") %> prohibited this test from being saved:</h5>
      <ul class="mb-0">
        <% test.errors.full_messages.each do |message| %>
          <li><%= message %></li>
        <% end %>
      </ul>
    </div>
  <% end %>

  <!-- Basic Info -->
  <div class="card mb-4">
    <div class="card-header">
      <h5 class="mb-0">Basic Information</h5>
    </div>
    <div class="card-body">
      <div class="row">
        <div class="col-lg-6 mb-3">
          <%= f.label :name, class: "form-label" %>
          <%= f.text_field :name, class: "form-control", placeholder: "e.g., test_greeting_response" %>
        </div>

        <div class="col-lg-6 mb-3">
          <%= f.label :enabled, class: "form-label" %>
          <div class="form-check mt-2">
            <%= f.check_box :enabled, class: "form-check-input" %>
            <%= f.label :enabled, "Enable this test", class: "form-check-label" %>
          </div>
        </div>
      </div>

      <div class="mb-3">
        <%= f.label :description, class: "form-label" %>
        <%= f.text_area :description, class: "form-control", rows: 3, placeholder: "Describe what this test validates..." %>
      </div>
    </div>
  </div>

  <!-- Evaluator Configs -->
  <div class="card mb-4" data-controller="evaluator-configs tooltip">
    <div class="card-header">
      <h5 class="mb-0">Evaluators (Optional)</h5>
    </div>
    <div class="card-body">
      <p class="text-muted small">Select evaluators to run for this test</p>

      <%
        # Get all evaluators (unified - no filtering needed)
        all_evaluators = PromptTracker::EvaluatorRegistry.all
        current_evaluators = test.evaluator_configs.to_a

        # Check if testable has vector stores attached (for file_search evaluator)
        testable = test.testable
        vector_store_ids = []

        if testable.is_a?(PromptTracker::Openai::Assistant)
          # Assistants store vector stores in metadata.tool_resources
          vector_store_ids = testable.metadata&.dig("tool_resources", "file_search", "vector_store_ids") || []
        elsif testable.is_a?(PromptTracker::PromptVersion)
          # PromptVersions store vector stores in model_config.tool_config (for Responses API)
          vector_store_ids = testable.model_config&.dig("tool_config", "file_search", "vector_store_ids") || []
        end

        has_vector_store = vector_store_ids.present?
      %>

      <div id="evaluators-list" class="row">
        <% all_evaluators.each do |key, meta| %>
          <% existing_config = current_evaluators.find { |e| e.evaluator_type == meta[:evaluator_class].name } %>
          <% is_selected = existing_config.present? %>
          <%
            # Disable file_search evaluator if no vector store attached
            is_disabled = (key.to_s == 'file_search' && !has_vector_store)
            disabled_reason = is_disabled ? "Attach a vector store to the assistant first" : nil
          %>

          <div class="col-lg-6 col-xl-4 mb-3">
            <div class="card h-100 evaluator-item <%= 'border-primary' if is_selected %> <%= 'opacity-50' if is_disabled %>" data-evaluator-key="<%= key %>">
              <div class="card-body p-3">
                <div class="form-check mb-2">
                  <input class="form-check-input"
                         type="checkbox"
                         id="evaluator_<%= key %>"
                         data-evaluator-key="<%= key %>"
                         data-evaluator-configs-target="checkbox"
                         data-action="change->evaluator-configs#toggleConfig"
                         <%= 'checked' if is_selected %>
                         <%= 'disabled' if is_disabled %>>
                  <label class="form-check-label w-100" for="evaluator_<%= key %>">
                    <div class="d-flex justify-content-between align-items-start">
                      <div>
                        <strong><i class="bi bi-<%= meta[:icon] %>"></i> <%= meta[:name] %></strong>
                        <br>
                        <small class="text-muted"><%= meta[:description] %></small>
                        <% if is_disabled %>
                          <br><small class="text-warning"><i class="bi bi-exclamation-triangle"></i> <%= disabled_reason %></small>
                        <% end %>
                      </div>
                    </div>
                  </label>
                </div>

                <div class="<%= 'collapse' unless is_selected %>"
                     id="config_<%= key %>"
                     data-evaluator-configs-target="config">
                  <hr class="my-2">
                  <div class="mt-2">
                    <label class="form-label small mb-1">Configuration</label>
                    <div data-evaluator-key="<%= key %>"
                          data-evaluator-configs-target="configFormContainer">
            <%= render partial: "prompt_tracker/evaluator_configs/forms/#{key}", locals: { existing_config: existing_config, evaluator_key: key, testable: test.testable } %>
                    </div>
                    <input type="hidden"
                            data-evaluator-key="<%= key %>"
                            data-evaluator-configs-target="configJson"
                            value='<%= JSON.generate(existing_config&.config || meta[:default_config] || {}) %>'>
                  </div>
                </div>
              </div>
            </div>
          </div>
        <% end %>
      </div>

      <%= f.hidden_field :evaluator_configs, id: "evaluator_configs_json", data: { evaluator_configs_target: "hiddenField" } %>
    </div>
  </div>

  <div class="d-flex justify-content-between">
    <%= f.submit test.new_record? ? "Create Test" : "Update Test", class: "btn btn-primary" %>
  </div>
<% end %>
