# Assistant Conversation Testing - Key Decisions & Trade-offs (UPDATED)

## ğŸ†• NEW DECISIONS (Based on User Requirements)

### 0.1. Unified Testable Index

**Decision:** Show all testables (Prompts + Assistants) in a single index view at `/testing`

**Alternatives:**
- A) Separate sections: `/testing/prompts` and `/testing/assistants`
- B) Unified index with filters (CHOSEN)
- C) Tabs within the same page

**Rationale:**
- âœ… **Single source of truth**: Users see all testables in one place
- âœ… **Better UX**: No navigation between sections
- âœ… **Consistent with polymorphic architecture**: Treats all testables equally
- âœ… **Easier comparison**: Compare pass rates across types
- âœ… **Simpler navigation**: One entry point

**Trade-offs:**
- âš ï¸ More complex controller logic (query multiple models)
- âš ï¸ Potential performance impact (mitigated with pagination)
- âœ… **Worth it**: Better UX > implementation complexity

---

### 0.2. Creation Wizard Modal

**Decision:** Use a modal wizard to guide users through testable creation

**Alternatives:**
- A) Separate "Create Prompt" and "Create Assistant" buttons
- B) Modal wizard with guided flow (CHOSEN)
- C) Dropdown menu

**Rationale:**
- âœ… **Educational**: Explains difference between testable types
- âœ… **Guided experience**: Reduces confusion
- âœ… **Contextual help**: Shows descriptions and examples
- âœ… **Flexible**: Easy to add more testable types
- âœ… **Clean UI**: Doesn't clutter interface

**Trade-offs:**
- âš ï¸ Extra click required
- âœ… **Worth it**: Better onboarding > speed for power users

---

### 0.3. Auto-fetch from OpenAI API

**Decision:** Automatically fetch assistant details from OpenAI API on show page load

**Alternatives:**
- A) Manual sync button only
- B) Auto-fetch on show page load (CHOSEN)
- C) Auto-fetch on creation only

**Rationale:**
- âœ… **Always up-to-date**: Latest assistant configuration
- âœ… **Better UX**: No manual action required
- âœ… **Transparent**: See what assistant actually does
- âœ… **Debugging**: Easier to spot configuration issues

**Trade-offs:**
- âš ï¸ Slower page load (mitigated with caching)
- âš ï¸ API rate limits (mitigated with last_synced_at check)
- âœ… **Worth it**: Accuracy > speed

**Implementation:**
- Cache for 5 minutes
- Show loading state
- Graceful degradation if API fails

---

### 0.4. Per-Message Scoring (NOT Criteria-Based)

**Decision:** Judge scores each assistant message individually (0-100) with reasons

**Alternatives:**
- A) Criteria-based scoring (Empathy: 90, Accuracy: 85, etc.)
- B) Per-message scoring (CHOSEN)
- C) Overall score only

**Rationale:**
- âœ… **Granular feedback**: Know exactly which messages were good/bad
- âœ… **Actionable**: Can improve specific responses
- âœ… **Simpler**: No need to define criteria upfront
- âœ… **Flexible**: Evaluation prompt can cover any aspect
- âœ… **Transparent**: See reasoning for each score

**Trade-offs:**
- âš ï¸ More LLM tokens (evaluating each message separately)
- âš ï¸ Longer evaluation time
- âœ… **Worth it**: Actionable feedback > speed

**Output format:**
```ruby
{
  overall_score: 88,  # Average of message scores
  message_scores: [
    { message_index: 0, role: "assistant", score: 90, reason: "Good opening" },
    { message_index: 2, role: "assistant", score: 85, reason: "Relevant questions" },
    { message_index: 4, role: "assistant", score: 90, reason: "Appropriate advice" }
  ],
  overall_feedback: "The assistant handled the conversation well..."
}
```

---

### 0.5. Global Configuration (NOT Row-Level)

**Decision:** Configure judge model globally in initializer, not per dataset row

**Alternatives:**
- A) Row-level evaluation config
- B) Test-level evaluation config (CHOSEN)
- C) Global configuration only

**Rationale:**
- âœ… **Simpler**: Less configuration overhead
- âœ… **Consistent**: All tests use same judge model
- âœ… **Easier to change**: Update one place, affects all tests
- âœ… **MVP-appropriate**: Can add row-level config later if needed

**Trade-offs:**
- âš ï¸ Less flexibility (can't use different models per scenario)
- âœ… **Worth it**: Simplicity > flexibility for MVP

**Configuration:**
```ruby
# config/initializers/prompt_tracker.rb
PromptTracker.configure do |config|
  config.conversation_judge_model = "gpt-4"
  config.user_simulator_model = "gpt-3.5-turbo"
end
```

---

### 0.6. Multiple Tests Per Assistant

**Decision:** Each assistant can have multiple tests with different evaluators

**Alternatives:**
- A) Multiple tests per assistant (CHOSEN)
- B) One test per assistant
- C) No tests (manual evaluation only)

**Rationale:**
- âœ… **More flexible**: Test different aspects of assistant behavior
- âœ… **Comprehensive testing**: Quality, tool usage, compliance, latency, etc.
- âœ… **Supports future evaluators**: ToolCallEvaluator, ResponseTimeEvaluator, ComplianceEvaluator
- âœ… **Real-world need**: Complex assistants need multiple evaluation strategies

**Examples:**
- **Test 1**: "Conversation Quality" with ConversationJudgeEvaluator
- **Test 2**: "Tool Usage" with ToolCallEvaluator (checks correct tools called)
- **Test 3**: "Response Time" with ResponseTimeEvaluator (checks latency)
- **Test 4**: "Compliance" with ComplianceEvaluator (checks for policy violations)

**Trade-offs:**
- âš ï¸ Slightly more complex UI (need "Create Test" button)
- âœ… **Worth it**: Flexibility needed for real-world assistant testing

**Implementation:**
- `has_many :tests, as: :testable` (no auto-creation)
- UI: "Create New Test" button on assistant show page
- Each test can have different evaluator_configs

---

### 0.7. Model Renames (PromptTest â†’ Test, PromptTestRun â†’ TestRun)

**Decision:** Rename models to reflect polymorphic nature

**Alternatives:**
- A) Keep PromptTest/PromptTestRun names
- B) Rename to Test/TestRun (CHOSEN)
- C) Create new models, deprecate old ones

**Rationale:**
- âœ… **Accurate naming**: Not just for prompts anymore
- âœ… **Cleaner code**: `Test` is shorter and clearer
- âœ… **Future-proof**: Works for any testable type
- âœ… **Consistent**: Matches polymorphic architecture

**Trade-offs:**
- âš ï¸ Migration complexity (rename tables, update references)
- âš ï¸ Breaking change for existing code
- âœ… **Worth it**: Better naming > migration effort

**Migration strategy:**
- Rename tables in migration
- Update all model references
- Update all view references
- Update all controller references
- Run comprehensive test suite

---

## ğŸ¯ Core Architecture Decision: Polymorphic Testable

### Decision
Use polymorphic association for tests instead of separate test types.

```ruby
# Instead of:
PromptTest belongs_to :prompt_version
AssistantTest belongs_to :assistant

# We use:
Test belongs_to :testable, polymorphic: true
# testable can be: PromptVersion OR Assistant
```

### Rationale
- âœ… **Extensible**: Can add new testable types (Workflow, Agent, RAG Pipeline) without refactoring
- âœ… **Consistent**: Same test infrastructure for all types
- âœ… **DRY**: Reuse test runs, evaluators, datasets
- âœ… **Future-proof**: Scales to any conversational AI system

### Trade-offs
- âš ï¸ Migration complexity: Need to backfill existing tests
- âš ï¸ Query complexity: Polymorphic queries are slightly more complex
- âœ… Worth it: Long-term flexibility outweighs short-term migration cost

---

## ğŸ¤– User Simulation Decision: LLM-Generated Conversations

### Decision
Use an LLM to simulate user behavior instead of scripted conversation turns.

```ruby
# Instead of:
dataset_row = {
  turn_1: "I have a headache",
  turn_2: "It's been 2 days",
  turn_3: "What should I do?"
}

# We use:
dataset_row = {
  user_prompt: "You are a patient with a severe headache that started 2 days ago..."
}
```

### Rationale
- âœ… **Natural**: Conversations feel human, not robotic
- âœ… **Adaptive**: User simulator responds to assistant's questions
- âœ… **Variety**: Same scenario generates different conversation paths
- âœ… **Easier**: Describe scenario vs. script every turn
- âœ… **Coverage**: Explores edge cases you didn't think of

### Trade-offs
- âš ï¸ **Non-deterministic**: Same test produces different results each run
- âš ï¸ **Cost**: Uses 2 LLMs per test (simulator + assistant) + judge
- âš ï¸ **Complexity**: Harder to debug when tests fail
- âœ… **Worth it**: Realistic testing > predictable testing

### Mitigation Strategies
- Use cheaper model for simulator (gpt-3.5-turbo)
- Run tests multiple times and aggregate results
- Store conversation_data for debugging
- Add max_turns limit to control cost

---

## ğŸ“Š Dataset Schema Decision: Flexible JSONB (SIMPLIFIED - Test Data Only)

### Decision
Reuse existing Dataset/DatasetRow models with flexible schema. **Dataset rows contain ONLY test scenario data, NO evaluation config.**

```ruby
# For PromptVersion tests:
row_data = { name: "Alice", issue: "billing" }

# For Assistant tests (ONLY scenario data):
row_data = {
  user_prompt: "You are a patient with a severe headache that started 2 days ago. You're worried it might be serious.",
  max_turns: 10
  # ONLY test scenario - NO evaluation_prompt, NO evaluation_config
}
```

**Evaluation config lives in Test â†’ EvaluatorConfig:**
```ruby
# Each test has its own evaluation strategy
test = assistant.tests.create!(
  name: "Conversation Quality Test",
  evaluator_configs_attributes: [{
    evaluator_type: :conversation_judge,
    config: {
      evaluation_prompt: "Evaluate each assistant message for empathy, accuracy, and professionalism..."
    }
  }]
)
```

### Rationale
- âœ… **Separation of concerns**: Dataset = test scenarios, Test = evaluation strategy
- âœ… **Reusable datasets**: Same dataset can be used by multiple tests with different evaluators
- âœ… **Simpler dataset creation**: Just describe the scenario, don't configure evaluation
- âœ… **Multiple tests per assistant**: Each test has its own EvaluatorConfig
- âœ… **No new tables**: Reuse existing infrastructure
- âœ… **Flexible**: Each testable type defines its own schema
- âœ… **Backward compatible**: Existing datasets still work
- âœ… **Simpler for MVP**: No row-level config complexity

### Trade-offs
- âš ï¸ **No schema validation**: Can't enforce structure at DB level
- âš ï¸ **Type confusion**: Same table stores different data shapes
- âš ï¸ **No row-level customization**: All rows use same evaluation config (MVP limitation)
- âœ… **Worth it**: Flexibility > strict schema

### Why NO row-level evaluation config?
- âœ… **Simpler**: Less configuration overhead
- âœ… **Consistent**: All scenarios evaluated the same way
- âœ… **MVP-appropriate**: Can add later if needed
- âœ… **Easier to understand**: Clear separation between data and evaluation

---

## ğŸ­ Evaluation Decision: Conversation Judge (LLM)

### Decision
Use LLM to evaluate entire conversation instead of per-message evaluators.

### Rationale
- âœ… **Holistic**: Evaluates conversation flow, not just individual responses
- âœ… **Nuanced**: Can judge empathy, tone, completeness
- âœ… **Flexible**: Custom criteria per test
- âœ… **Realistic**: Mimics how humans evaluate conversations

### Trade-offs
- âš ï¸ **Cost**: LLM call per evaluation
- âš ï¸ **Latency**: Slower than rule-based evaluators
- âš ï¸ **Variability**: LLM judge may be inconsistent
- âœ… **Worth it**: Quality evaluation > fast evaluation

### Mitigation Strategies
- Use lower temperature (0.3) for consistency
- Cache judge responses
- Allow custom judge models (cheaper options)
- Provide clear criteria to reduce variability

---

## ğŸ”„ Thread Management Decision: New Thread Per Test

### Decision (MVP)
Always create new thread for each test run. No thread reuse.

### Rationale
- âœ… **Simple**: No thread lifecycle management
- âœ… **Isolated**: Each test is independent
- âœ… **Predictable**: No state leakage between tests
- âœ… **MVP-friendly**: Defer complexity to post-MVP

### Trade-offs
- âš ï¸ **Can't test context retention**: Each conversation starts fresh
- âš ï¸ **Higher cost**: Thread creation overhead
- âœ… **Worth it for MVP**: Simplicity > feature completeness

### Post-MVP Enhancement
- Add `thread_id` field to tests
- Add "reuse thread" checkbox
- Store thread_id in test run metadata
- Allow testing multi-session conversations

---

## ğŸ¨ UI Decision: Separate Assistant Section

### Decision
Create dedicated `/testing/assistants` section instead of mixing with prompts.

### Rationale
- âœ… **Clear separation**: Different mental models (prompts vs assistants)
- âœ… **Focused UX**: Purpose-built for conversation testing
- âœ… **Less confusion**: No conditional UI based on testable type
- âœ… **Easier to build**: Separate controllers/views

### Trade-offs
- âš ï¸ **More code**: Duplicate some UI patterns
- âš ï¸ **Navigation complexity**: Two testing sections
- âœ… **Worth it**: Clarity > code reuse

---

## ğŸš« What's NOT in MVP

### 1. Playground Integration
**Why not:** Playground is for prompt drafting (single-turn). Assistants need multi-turn UI.
**Post-MVP:** Build separate "Assistant Playground" with chat interface.

### 2. Tool Call Handling
**Why not:** Adds significant complexity (mock tools, output submission).
**Post-MVP:** Add tool call mocking and automatic output submission.

### 3. Deterministic Conversations
**Why not:** Requires seeding, conversation replay, complex state management.
**Post-MVP:** Add seed parameter to user simulator for regression tests.

### 4. Pre-built Personas
**Why not:** Requires persona library, management UI, categorization.
**Post-MVP:** Build persona library with common user types.

### 5. Real-time Preview
**Why not:** Requires WebSocket/SSE, streaming UI, complex state.
**Post-MVP:** Add live conversation preview in dataset creation.

---

## ğŸ“ˆ Success Metrics

### MVP Success = Can Answer These Questions:
1. âœ… Can I test an OpenAI assistant with realistic conversations?
2. âœ… Can I define user scenarios without scripting every turn?
3. âœ… Can I evaluate conversation quality holistically?
4. âœ… Can I see the full conversation in test results?
5. âœ… Can I run multiple scenarios against one assistant?

### Post-MVP Success = Can Also Answer:
- Can I test multi-session conversations (thread reuse)?
- Can I test assistants with tool calls?
- Can I create deterministic regression tests?
- Can I preview conversations before running tests?
- Can I analyze conversation patterns across tests?

---

## ğŸ¯ MVP Scope Summary

| Feature | MVP | Post-MVP |
|---------|-----|----------|
| Polymorphic tests | âœ… | - |
| LLM user simulator | âœ… | - |
| Conversation judge | âœ… | - |
| New thread per test | âœ… | Thread reuse |
| Basic UI | âœ… | Advanced UI |
| Assistant sync from config | âœ… | Assistant CRUD |
| Error handling | âœ… | - |
| Integration tests | âœ… | - |
| Tool calls | âŒ | âœ… |
| Deterministic mode | âŒ | âœ… |
| Persona library | âŒ | âœ… |
| Real-time preview | âŒ | âœ… |
| Playground integration | âŒ | âœ… |

---

**This MVP gives you 80% of the value with 20% of the complexity.** ğŸš€
