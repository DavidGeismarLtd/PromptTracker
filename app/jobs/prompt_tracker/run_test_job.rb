# frozen_string_literal: true

module PromptTracker
  # Background job to run a single prompt test.
  #
  # This job:
  # 1. Creates a PromptTestRun with status "running"
  # 2. Broadcasts "test started" event via ActionCable
  # 3. Executes the LLM call (real or mock)
  # 4. Creates the LlmResponse
  # 5. Enqueues RunEvaluatorsJob to run evaluators asynchronously
  #
  # @example Enqueue a test run
  #   RunTestJob.perform_later(
  #     test_id: 123,
  #     version_id: 456,
  #     use_real_llm: true,
  #     metadata: { triggered_by: "run_all" }
  #   )
  #
  class RunTestJob < ApplicationJob
    queue_as :prompt_tracker_tests

    # Execute the test run
    #
    # @param test_id [Integer] ID of the PromptTest to run
    # @param version_id [Integer] ID of the PromptVersion to test against
    # @param use_real_llm [Boolean] whether to use real LLM API or mock
    # @param metadata [Hash] additional metadata for the test run
    def perform(test_id, version_id, use_real_llm: false, metadata: {})
      Rails.logger.info "ðŸš€ RunTestJob started for test #{test_id}, version #{version_id}"

      test = PromptTest.find(test_id)
      version = PromptVersion.find(version_id)

      # Use PromptTestRunner to execute the test
      runner = PromptTestRunner.new(test, version, metadata: metadata)

      # Run the test with appropriate LLM call
      runner.run_async! do |rendered_prompt|
        if use_real_llm
          call_real_llm(rendered_prompt, test.model_config)
        else
          generate_mock_llm_response(rendered_prompt, test.model_config)
        end
      end

      Rails.logger.info "âœ… RunTestJob completed for test #{test_id}"
    rescue StandardError => e
      Rails.logger.error "âŒ RunTestJob failed for test #{test_id}: #{e.message}"
      Rails.logger.error e.backtrace.join("\n")

      # Create error test run if one doesn't exist
      test_run = PromptTestRun.create!(
        prompt_test_id: test_id,
        prompt_version_id: version_id,
        status: "error",
        error_message: "#{e.class}: #{e.message}",
        metadata: metadata.merge(error: true, error_class: e.class.name),
        evaluator_results: [],
        assertion_results: {}
      )

      # Broadcast error to PromptVersionChannel
      broadcast_test_error(test_run)

      raise # Re-raise to let Sidekiq handle it (no retry for now)
    end

    private

    # Call real LLM API
    #
    # @param rendered_prompt [String] the rendered prompt
    # @param model_config [Hash] the model configuration
    # @return [RubyLLM::Message] LLM API response
    def call_real_llm(rendered_prompt, model_config)
      config = model_config.with_indifferent_access
      provider = config[:provider] || "openai"
      model = config[:model] || "gpt-4"
      temperature = config[:temperature] || 0.7
      max_tokens = config[:max_tokens]

      Rails.logger.info "ðŸ”§ Calling REAL LLM: #{provider}/#{model}"

      LlmClientService.call(
        provider: provider,
        model: model,
        prompt: rendered_prompt,
        temperature: temperature,
        max_tokens: max_tokens
      )[:raw] # Return raw RubyLLM::Message
    end

    # Generate a mock LLM response for testing
    #
    # @param rendered_prompt [String] the rendered prompt
    # @param model_config [Hash] the model configuration
    # @return [Hash] mock LLM response in OpenAI format
    def generate_mock_llm_response(rendered_prompt, model_config)
      provider = model_config["provider"] || model_config[:provider] || "openai"

      Rails.logger.info "ðŸŽ­ Generating MOCK LLM response for #{provider}"

      # Generate a realistic mock response based on the prompt
      mock_text = "This is a mock response to: #{rendered_prompt.truncate(100)}\n\n"
      mock_text += "In a production environment, this would be replaced with an actual API call to #{provider}.\n"
      mock_text += "The response would be generated by the configured model and would address the prompt appropriately."

      # Return in OpenAI-like format for compatibility
      {
        "choices" => [
          {
            "message" => {
              "content" => mock_text
            }
          }
        ]
      }
    end

    # Broadcast test error to PromptVersionChannel
    #
    # @param test_run [PromptTestRun] the failed test run
    def broadcast_test_error(test_run)
      PromptVersionChannel.broadcast_to(
        test_run.prompt_version,
        {
          test_id: test_run.prompt_test_id,
          test_run_id: test_run.id,
          status: "error",
          error_message: test_run.error_message
        }
      )
    end
  end
end

