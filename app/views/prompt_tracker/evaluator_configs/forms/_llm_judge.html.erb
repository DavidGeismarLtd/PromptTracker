<%# Unified form for LLM Judge Evaluator - works for both test configuration and manual evaluation %>
<% existing_config ||= nil %>
<% config = existing_config&.config || {} %>
<% evaluator_key ||= 'llm_judge' %>
<% namespace ||= 'config' %>
<% judge_model = config['judge_model'] || config[:judge_model] || 'gpt-4o' %>

<div class="alert alert-info mb-3">
  <i class="bi bi-robot"></i>
  <strong>LLM Judge</strong>
  <p class="mb-0 mt-1">Uses an LLM to evaluate response quality. Returns a score (0-100) based on custom instructions.</p>
</div>

<!-- Judge Model -->
<div class="mb-3">
  <label class="form-label">Judge Model</label>
  <%
    # Get all available models from configuration
    all_models = PromptTracker.configuration.available_models
  %>
  <select name="<%= namespace %>[judge_model]" class="form-select" required>
    <% all_models.each do |provider_key, models| %>
      <% provider_name = provider_key.to_s.titleize %>
      <% # Only show provider if API key is configured %>
      <% if provider_api_key_present?(provider_key.to_s) %>
        <% # Group models by category %>
        <% models.group_by { |m| m[:category] }.each do |category, models_in_category| %>
          <% if category.present? %>
            <optgroup label="<%= provider_name %> - <%= category %>">
          <% else %>
            <optgroup label="<%= provider_name %>">
          <% end %>

          <% models_in_category.each do |model| %>
            <option value="<%= model[:id] %>"
                    <%= 'selected' if judge_model == model[:id] %>>
              <%= model[:name] || model[:id] %>
            </option>
          <% end %>

          </optgroup>
        <% end %>
      <% end %>
    <% end %>
  </select>
  <small class="form-text text-muted">
    The LLM model to use as a judge. Only models with configured API keys are shown.
  </small>
</div>

<!-- Custom Instructions -->
<div class="mb-3">
  <label class="form-label">Custom Instructions</label>
  <textarea name="<%= namespace %>[custom_instructions]"
            class="form-control"
            rows="4"
            placeholder="Define what the evaluator assesses (e.g., 'Evaluate as a customer support manager' or 'Focus on technical correctness for a senior developer audience')"
            required><%= config['custom_instructions'] || config[:custom_instructions] || '' %></textarea>
  <small class="form-text text-muted">
    Provide specific instructions for the LLM judge on how to evaluate the response.
  </small>
</div>

<!-- Threshold Score -->
<div class="mb-3">
  <label class="form-label">Threshold Score (0-100)</label>
  <input type="number"
         name="<%= namespace %>[threshold_score]"
         class="form-control"
         value="<%= config['threshold_score'] || config[:threshold_score] || 70 %>"
         min="0"
         max="100"
         required>
  <small class="form-text text-muted">
    Minimum score required to pass (0-100). The LLM judge will return a score from 0-100, and if it's >= this threshold, the evaluation passes.
  </small>
</div>

<div class="alert alert-warning">
  <i class="bi bi-exclamation-triangle"></i>
  <strong>Note:</strong> LLM judge evaluations require API calls and may run asynchronously. Make sure you have configured your API keys in the <code>.env</code> file.
</div>

<div class="alert alert-secondary">
  <strong>Scoring:</strong>
  <ul class="mb-0 mt-2">
    <li>The LLM judge returns a score from 0-100 based on your custom instructions</li>
    <li>If score >= threshold: <strong>PASS</strong></li>
    <li>If score < threshold: <strong>FAIL</strong></li>
  </ul>
</div>
